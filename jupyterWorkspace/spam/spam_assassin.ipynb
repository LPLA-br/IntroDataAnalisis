{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db26b858-a831-48a6-86b7-92b32c244270",
   "metadata": {},
   "source": [
    "# Filtro de spam\n",
    "## Carregamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f9b886-0005-4e35-b0fa-3b272fab1ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From gort44@excite.com Mon Jun 24 17:54:21 200...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From fork-admin@xent.com Mon Jul 29 11:39:57 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From dcm123@btamail.net.cn Mon Jun 24 17:49:23...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...       0\n",
       "1  From gort44@excite.com Mon Jun 24 17:54:21 200...       1\n",
       "2  From fork-admin@xent.com Mon Jul 29 11:39:57 2...       1\n",
       "3  From dcm123@btamail.net.cn Mon Jun 24 17:49:23...       1\n",
       "4  From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar o dataset Boston Housing\n",
    "df = pd.read_csv('spam_assassin.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca236f06-8fcf-4ecd-a251-72dc957483e9",
   "metadata": {},
   "source": [
    "## Pré-processamento - Limpeza - Normalização\n",
    "\n",
    "### Conceitos básicos\n",
    "\n",
    "#### Tokenização\n",
    "Tokenização é o processo de dividir um texto em unidades menores, chamadas tokens, que podem ser palavras, frases, ou até caracteres individuais, dependendo do contexto e objetivo da análise. Em processamento de linguagem natural (PLN), a tokenização é uma etapa essencial para transformar texto bruto em dados estruturados, permitindo que as palavras ou frases sejam analisadas separadamente. A tokenização facilita operações subsequentes, como análise de frequência, remoção de stopwords, e representação vetorial, ajudando algoritmos de aprendizado de máquina a entender e processar texto de maneira mais eficiente.\n",
    "\n",
    "#### Stopwords\n",
    "Stopwords são palavras comuns em um idioma que, por si só, geralmente não carregam muito significado para a análise de texto, como \"e\", \"de\", \"para\" e \"o\" em português, ou \"and\", \"the\", \"of\" e \"in\" em inglês. No processamento de linguagem natural, as stopwords são frequentemente removidas dos dados para reduzir o ruído e melhorar o desempenho dos algoritmos, concentrando a análise nas palavras que carregam mais informações relevantes. Excluir essas palavras ajuda a simplificar o texto e diminuir a dimensionalidade dos dados, facilitando tarefas como classificação de texto, análise de sentimento e recuperação de informações.\n",
    "\n",
    "\n",
    "#### Stemming\n",
    "Stemming é o processo de reduzir palavras às suas formas base ou radicais, removendo sufixos e outras partes flexionadas. Essa técnica, usada em processamento de linguagem natural, simplifica o texto ao agrupar variações morfológicas de uma palavra (como plurais, tempos verbais, etc.) em um único termo comum, sem considerar o contexto completo. Ferramentas de stemming, como o PorterStemmer, ajudam a normalizar o vocabulário, diminuindo a dimensionalidade e a complexidade dos dados textuais, o que facilita tarefas como classificação de texto e recuperação de informação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2b8492-b81a-454a-8d57-d74fb6226a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Baixar stopwords e tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# o dataset está em inglês, então serão consideradas as \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remover_cabecalhos(email_texto):\n",
    "    # Remove todas as linhas de cabeçalhos que começam com um padrão de cabeçalho típico de email\n",
    "    # \"^\" indica o início da linha e \"(?i)\" torna a regex case-insensitive\n",
    "    email_sem_cabecalhos = re.sub(r\"(?i)^(From|To|Date|Subject|Reply-To|Message-ID|MIME-Version|Content-Type|Content-Transfer-Encoding|Received|X-\\w+):.*\\n?\", '', email_texto, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove possíveis linhas em branco após a remoção dos cabeçalhos\n",
    "    email_sem_cabecalhos = email_sem_cabecalhos.strip()\n",
    "    \n",
    "    return email_sem_cabecalhos\n",
    "\n",
    "def limpar_texto(texto):\n",
    "    # Converte o texto para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remove cabeçalhos de e-mail\n",
    "    texto = remover_cabecalhos(texto)\n",
    "    \n",
    "    # Remove URLs\n",
    "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove as pontuações\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remova números usando expressões regulares\n",
    "    texto = re.sub(r'\\d+', '', texto)\n",
    "    \n",
    "    # Tokenização\n",
    "    palavras = word_tokenize(texto)\n",
    "    \n",
    "    # Remove stopwords e aplica o stemming\n",
    "    palavras_limpa = [stemmer.stem(palavra) for palavra in palavras if palavra not in stop_words]\n",
    "    \n",
    "    # Reconstrue o texto limpo\n",
    "    texto_limpo = ' '.join(palavras_limpa)\n",
    "    \n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135ea98b-62b4-465b-811c-44b654a9c818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>ilugadminlinuxi mon jul returnpath ilugadminli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From gort44@excite.com Mon Jun 24 17:54:21 200...</td>\n",
       "      <td>1</td>\n",
       "      <td>gortexcitecom mon jun returnpath gortexcitecom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From fork-admin@xent.com Mon Jul 29 11:39:57 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>forkadminxentcom mon jul returnpath forkadminx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From dcm123@btamail.net.cn Mon Jun 24 17:49:23...</td>\n",
       "      <td>1</td>\n",
       "      <td>dcmbtamailnetcn mon jun returnpath dcmbtamailn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>ilugadminlinuxi mon aug returnpath ilugadminli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  From ilug-admin@linux.ie Mon Jul 29 11:28:02 2...       0   \n",
       "1  From gort44@excite.com Mon Jun 24 17:54:21 200...       1   \n",
       "2  From fork-admin@xent.com Mon Jul 29 11:39:57 2...       1   \n",
       "3  From dcm123@btamail.net.cn Mon Jun 24 17:49:23...       1   \n",
       "4  From ilug-admin@linux.ie Mon Aug 19 11:02:47 2...       0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  ilugadminlinuxi mon jul returnpath ilugadminli...  \n",
       "1  gortexcitecom mon jun returnpath gortexcitecom...  \n",
       "2  forkadminxentcom mon jul returnpath forkadminx...  \n",
       "3  dcmbtamailnetcn mon jun returnpath dcmbtamailn...  \n",
       "4  ilugadminlinuxi mon aug returnpath ilugadminli...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_clean'] = df['text'].apply(limpar_texto)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9263329-bd50-4fff-8bce-458ca13073e1",
   "metadata": {},
   "source": [
    "## Pré-processamento - Transformação\n",
    "\n",
    "### Representação de texto\n",
    "#### Bag of Words\n",
    "A técnica Bag of Words (BoW) é um método simples de representação de texto que transforma documentos em uma matriz de contagem de palavras, ignorando a ordem das palavras e focando na frequência de cada termo. Cada coluna da matriz representa uma palavra única no corpus, e cada célula indica o número de vezes que essa palavra aparece em um determinado documento. Apesar de sua simplicidade, o BoW é eficaz em capturar a presença de termos e é amplamente utilizado em tarefas de processamento de linguagem natural, como classificação de texto e análise de sentimento. No entanto, o BoW não considera contexto ou relações semânticas entre as palavras, limitando sua capacidade de capturar nuances de significado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f133356a-f63f-44f0-b5c7-cb79628dd492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaqaaaaia</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaacqaaayqb</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaghoawajwa</th>\n",
       "      <th>...</th>\n",
       "      <th>zzzzteanaegroup</th>\n",
       "      <th>zzzzteanaegroupscom</th>\n",
       "      <th>zzzzteanataintorg</th>\n",
       "      <th>zzzzteanayahoogroupscom</th>\n",
       "      <th>zzzzuseperlspamassassintaintorg</th>\n",
       "      <th>zzzzvfmtyfgwtqbhmwzuffranxlydunhsdtqqprryuelttfrpjpoqgeartzjiaj</th>\n",
       "      <th>zzzzvpexofhibogkbjdemkenzxddoadomnkmbplmazkpuaeptcmaewxfeawaaamacgcfvti</th>\n",
       "      <th>zzzzwebdevspamassassintaintorg</th>\n",
       "      <th>zzzzwebnotenet</th>\n",
       "      <th>zzzzyour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91842 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaa  aaaaaaaaaa  aaaaaaaaaaaaaaaaaaaaa  \\\n",
       "0   0    0     0           0                      0   \n",
       "1   0    0     0           0                      0   \n",
       "2   0    0     0           0                      0   \n",
       "3   0    0     0           0                      0   \n",
       "4   0    0     0           0                      0   \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
       "0                                                  0                \n",
       "1                                                  0                \n",
       "2                                                  0                \n",
       "3                                                  0                \n",
       "4                                                  0                \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
       "0                                                  0                              \n",
       "1                                                  0                              \n",
       "2                                                  0                              \n",
       "3                                                  0                              \n",
       "4                                                  0                              \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaqaaaaia  \\\n",
       "0                                                  0                              \n",
       "1                                                  0                              \n",
       "2                                                  0                              \n",
       "3                                                  0                              \n",
       "4                                                  0                              \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaacqaaayqb  \\\n",
       "0                                                  0                              \n",
       "1                                                  0                              \n",
       "2                                                  0                              \n",
       "3                                                  0                              \n",
       "4                                                  0                              \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaghoawajwa  \\\n",
       "0                                                  0                             \n",
       "1                                                  0                             \n",
       "2                                                  0                             \n",
       "3                                                  0                             \n",
       "4                                                  0                             \n",
       "\n",
       "   ...  zzzzteanaegroup  zzzzteanaegroupscom  zzzzteanataintorg  \\\n",
       "0  ...                0                    0                  0   \n",
       "1  ...                0                    0                  0   \n",
       "2  ...                0                    0                  0   \n",
       "3  ...                0                    0                  0   \n",
       "4  ...                0                    0                  0   \n",
       "\n",
       "   zzzzteanayahoogroupscom  zzzzuseperlspamassassintaintorg  \\\n",
       "0                        0                                0   \n",
       "1                        0                                0   \n",
       "2                        0                                0   \n",
       "3                        0                                0   \n",
       "4                        0                                0   \n",
       "\n",
       "   zzzzvfmtyfgwtqbhmwzuffranxlydunhsdtqqprryuelttfrpjpoqgeartzjiaj  \\\n",
       "0                                                  0                 \n",
       "1                                                  0                 \n",
       "2                                                  0                 \n",
       "3                                                  0                 \n",
       "4                                                  0                 \n",
       "\n",
       "   zzzzvpexofhibogkbjdemkenzxddoadomnkmbplmazkpuaeptcmaewxfeawaaamacgcfvti  \\\n",
       "0                                                  0                         \n",
       "1                                                  0                         \n",
       "2                                                  0                         \n",
       "3                                                  0                         \n",
       "4                                                  0                         \n",
       "\n",
       "   zzzzwebdevspamassassintaintorg  zzzzwebnotenet  zzzzyour  \n",
       "0                               0               0         0  \n",
       "1                               0               0         0  \n",
       "2                               0               0         0  \n",
       "3                               0               0         0  \n",
       "4                               0               0         0  \n",
       "\n",
       "[5 rows x 91842 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Inicializa o vetorizador Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(df[\"text_clean\"])\n",
    "\n",
    "# Converte a matriz esparsa para um DataFrame para visualização\n",
    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_bow.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8f939-f2d3-4d23-a2d9-aac2541ef27b",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "A técnica TF-IDF (Term Frequency - Inverse Document Frequency) é uma representação vetorial que pondera cada termo com base em sua frequência no documento e na relevância dentro do corpus, destacando palavras distintivas de cada documento. A frequência de um termo (TF) mede quantas vezes ele aparece no documento, enquanto a frequência inversa de documentos (IDF) reduz o peso de palavras muito comuns no corpus, como artigos ou preposições. Dessa forma, a TF-IDF atribui valores mais altos a palavras que são importantes para um documento específico, mas que não são frequentes no conjunto geral de documentos, o que torna essa técnica especialmente útil para tarefas de recuperação de informação e classificação de texto, onde é importante capturar a relevância de termos específicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cdc6feb-4b17-454a-8ec7-1347af61e390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaqaaaaia</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaacqaaayqb</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaghoawajwa</th>\n",
       "      <th>...</th>\n",
       "      <th>zzzzteanaegroup</th>\n",
       "      <th>zzzzteanaegroupscom</th>\n",
       "      <th>zzzzteanataintorg</th>\n",
       "      <th>zzzzteanayahoogroupscom</th>\n",
       "      <th>zzzzuseperlspamassassintaintorg</th>\n",
       "      <th>zzzzvfmtyfgwtqbhmwzuffranxlydunhsdtqqprryuelttfrpjpoqgeartzjiaj</th>\n",
       "      <th>zzzzvpexofhibogkbjdemkenzxddoadomnkmbplmazkpuaeptcmaewxfeawaaamacgcfvti</th>\n",
       "      <th>zzzzwebdevspamassassintaintorg</th>\n",
       "      <th>zzzzwebnotenet</th>\n",
       "      <th>zzzzyour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91842 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aaaa  aaaaaaaaaa  aaaaaaaaaaaaaaaaaaaaa  \\\n",
       "0  0.0  0.0   0.0         0.0                    0.0   \n",
       "1  0.0  0.0   0.0         0.0                    0.0   \n",
       "2  0.0  0.0   0.0         0.0                    0.0   \n",
       "3  0.0  0.0   0.0         0.0                    0.0   \n",
       "4  0.0  0.0   0.0         0.0                    0.0   \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
       "0                                                0.0                \n",
       "1                                                0.0                \n",
       "2                                                0.0                \n",
       "3                                                0.0                \n",
       "4                                                0.0                \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  \\\n",
       "0                                                0.0                              \n",
       "1                                                0.0                              \n",
       "2                                                0.0                              \n",
       "3                                                0.0                              \n",
       "4                                                0.0                              \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaqaaaaia  \\\n",
       "0                                                0.0                              \n",
       "1                                                0.0                              \n",
       "2                                                0.0                              \n",
       "3                                                0.0                              \n",
       "4                                                0.0                              \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaacqaaayqb  \\\n",
       "0                                                0.0                              \n",
       "1                                                0.0                              \n",
       "2                                                0.0                              \n",
       "3                                                0.0                              \n",
       "4                                                0.0                              \n",
       "\n",
       "   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaghoawajwa  \\\n",
       "0                                                0.0                             \n",
       "1                                                0.0                             \n",
       "2                                                0.0                             \n",
       "3                                                0.0                             \n",
       "4                                                0.0                             \n",
       "\n",
       "   ...  zzzzteanaegroup  zzzzteanaegroupscom  zzzzteanataintorg  \\\n",
       "0  ...              0.0                  0.0                0.0   \n",
       "1  ...              0.0                  0.0                0.0   \n",
       "2  ...              0.0                  0.0                0.0   \n",
       "3  ...              0.0                  0.0                0.0   \n",
       "4  ...              0.0                  0.0                0.0   \n",
       "\n",
       "   zzzzteanayahoogroupscom  zzzzuseperlspamassassintaintorg  \\\n",
       "0                      0.0                              0.0   \n",
       "1                      0.0                              0.0   \n",
       "2                      0.0                              0.0   \n",
       "3                      0.0                              0.0   \n",
       "4                      0.0                              0.0   \n",
       "\n",
       "   zzzzvfmtyfgwtqbhmwzuffranxlydunhsdtqqprryuelttfrpjpoqgeartzjiaj  \\\n",
       "0                                                0.0                 \n",
       "1                                                0.0                 \n",
       "2                                                0.0                 \n",
       "3                                                0.0                 \n",
       "4                                                0.0                 \n",
       "\n",
       "   zzzzvpexofhibogkbjdemkenzxddoadomnkmbplmazkpuaeptcmaewxfeawaaamacgcfvti  \\\n",
       "0                                                0.0                         \n",
       "1                                                0.0                         \n",
       "2                                                0.0                         \n",
       "3                                                0.0                         \n",
       "4                                                0.0                         \n",
       "\n",
       "   zzzzwebdevspamassassintaintorg  zzzzwebnotenet  zzzzyour  \n",
       "0                             0.0             0.0       0.0  \n",
       "1                             0.0             0.0       0.0  \n",
       "2                             0.0             0.0       0.0  \n",
       "3                             0.0             0.0       0.0  \n",
       "4                             0.0             0.0       0.0  \n",
       "\n",
       "[5 rows x 91842 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicializa o vetorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"text_clean\"])\n",
    "\n",
    "# Converte a matriz esparsa para um DataFrame para visualização\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df_tfidf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0da48b-4d21-4019-b28d-408c114c8e27",
   "metadata": {},
   "source": [
    "## Classificação\n",
    "\n",
    "### Antes de tudo, como interpretar os resultados\n",
    "**Precision, Recall, F1-Score e Support** são métricas de avaliação para modelos de classificação, especialmente úteis em tarefas de classificação binária e multilabel.\n",
    "\n",
    "- **Precision (Precisão)**: Mede a proporção de predições positivas corretas em relação ao total de predições positivas feitas pelo modelo. Em um contexto de spam, uma alta precisão significa que a maioria dos emails classificados como spam realmente são spam.\n",
    "\n",
    "- **Recall (Sensibilidade ou Revocação)**: Mede a proporção de predições positivas corretas em relação ao total de positivos reais no conjunto de dados. Em termos de spam, uma alta recall significa que o modelo identificou a maioria dos emails de spam existentes.\n",
    "\n",
    "- **F1-Score**: É a média harmônica entre precisão e recall, equilibrando as duas métricas. É útil quando há um trade-off entre precisão e recall, especialmente em casos onde ambas são importantes.\n",
    "\n",
    "- **Support**: Representa o número total de ocorrências de cada classe no conjunto de dados de teste (ex.: o número total de emails de spam e não-spam). Isso é importante para entender o equilíbrio de classes e o desempenho do modelo em cada uma.\n",
    "\n",
    "\n",
    "### Usando o modelo clássico de Naive Bayes\n",
    "**Naive Bayes** é um modelo probabilístico baseado no Teorema de Bayes, amplamente utilizado para classificação de textos, como na filtragem de spam. A principal suposição do Naive Bayes é a de que as características (ou palavras, no caso de texto) são independentes entre si, o que significa que a presença de uma palavra não influencia a presença de outra. Essa simplificação torna o Naive Bayes rápido e eficiente, especialmente em problemas de texto, mesmo que essa independência completa raramente seja verdadeira. Existem várias variantes do Naive Bayes, sendo a Multinomial Naive Bayes a mais comum para problemas de classificação de texto, onde a frequência das palavras (ou tokens) é levada em consideração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c9fae7-f49b-4967-a58c-2202b8f02d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos dados em 2 conjuntos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividindo os dados em treino e teste (80% treino, 20% teste) - bow\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_matrix, df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Dividindo os dados em treino e teste (80% treino, 20% teste) - tfidf\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(df_tfidf, df['target'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748a4b09-9cf4-4cfd-b8e5-fe661b769ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Acurácia: 0.9474137931034483\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       779\n",
      "           1       1.00      0.84      0.91       381\n",
      "\n",
      "    accuracy                           0.95      1160\n",
      "   macro avg       0.96      0.92      0.94      1160\n",
      "weighted avg       0.95      0.95      0.95      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Modelo Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "# Previsões e Avaliação\n",
    "y_pred_nb = nb_model.predict(X_test_bow)\n",
    "print(\"Naive Bayes - Acurácia:\", accuracy_score(y_test_bow, y_pred_nb))\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test_bow, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b700d59f-0637-4ac4-bfcd-78760be92124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Acurácia: 0.8853448275862069\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       779\n",
      "           1       1.00      0.65      0.79       381\n",
      "\n",
      "    accuracy                           0.89      1160\n",
      "   macro avg       0.93      0.83      0.85      1160\n",
      "weighted avg       0.90      0.89      0.88      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Modelo Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Previsões e Avaliação\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "print(\"Naive Bayes - Acurácia:\", accuracy_score(y_test_tfidf, y_pred_nb))\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test_tfidf, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cccfd3-01c2-42d4-b248-323eafeb4b5f",
   "metadata": {},
   "source": [
    "### Regressão logística\n",
    "**Regressão Logística** é um modelo de classificação linear que usa a função logística (ou sigmoid) para mapear a soma ponderada das características de entrada para uma probabilidade entre 0 e 1. Diferente do Naive Bayes, a Regressão Logística não assume independência entre as características, o que pode tornar o modelo mais flexível em cenários onde as variáveis têm alguma correlação. É comumente usado em problemas binários, como detectar se um email é spam ou não. A Regressão Logística otimiza um conjunto de pesos para cada característica, ajustando o modelo para maximizar a probabilidade dos dados observados, fornecendo uma classificação robusta para tarefas de texto e outros tipos de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "175b7a39-efaa-4fcb-bbfe-181fd58e15f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressão Logística - Acurácia: 0.996551724137931\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       779\n",
      "           1       1.00      0.99      0.99       381\n",
      "\n",
      "    accuracy                           1.00      1160\n",
      "   macro avg       1.00      0.99      1.00      1160\n",
      "weighted avg       1.00      1.00      1.00      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Modelo de Regressão Logística\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "# Previsões e Avaliação\n",
    "y_pred_lr = lr_model.predict(X_test_bow)\n",
    "print(\"Regressão Logística - Acurácia:\", accuracy_score(y_test_bow, y_pred_lr))\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test_bow, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b51fb026-76fa-45dc-9d26-15ef5470106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressão Logística - Acurácia: 0.8974137931034483\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       779\n",
      "           1       1.00      0.69      0.81       381\n",
      "\n",
      "    accuracy                           0.90      1160\n",
      "   macro avg       0.93      0.84      0.87      1160\n",
      "weighted avg       0.91      0.90      0.89      1160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Modelo de Regressão Logística\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Previsões e Avaliação\n",
    "y_pred_lr = lr_model.predict(X_test_bow)\n",
    "print(\"Regressão Logística - Acurácia:\", accuracy_score(y_test_tfidf, y_pred_lr))\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test_tfidf, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f1e7c-3da1-471e-888e-ac3b75b83053",
   "metadata": {},
   "source": [
    "### MLP (Multi Layer Perceptron)\n",
    "**Classificação com MLP (Perceptron Multicamada)** usa uma rede neural artificial com múltiplas camadas de neurônios para capturar padrões complexos nos dados, incluindo relações não lineares entre as características. Em uma MLP, a informação passa pela camada de entrada, uma ou mais camadas ocultas e a camada de saída, onde cada neurônio aplica uma função de ativação para ajudar a rede a aprender padrões de classificação. A MLP é treinada usando algoritmos de retropropagação, ajustando os pesos das conexões para minimizar o erro de predição. Esse modelo é particularmente útil para dados mais complexos, mas exige mais tempo de treinamento e processamento, além de necessitar de uma quantidade maior de dados em comparação com modelos lineares como Naive Bayes e Regressão Logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1cc490-2088-4d75-95ae-a1d21d0d98ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Acurácia: 0.9974137931034482\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       779\n",
      "           1       1.00      0.99      1.00       381\n",
      "\n",
      "    accuracy                           1.00      1160\n",
      "   macro avg       1.00      1.00      1.00      1160\n",
      "weighted avg       1.00      1.00      1.00      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Modelo de MLP\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=300, random_state=42)\n",
    "mlp_model.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "# Previsões e Avaliação\n",
    "y_pred_mlp = mlp_model.predict(X_test_bow)\n",
    "print(\"MLP - Acurácia:\", accuracy_score(y_test_bow, y_pred_mlp))\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test_bow, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "003ce755-7fd8-429c-98f6-7f150d937f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Acurácia: 0.9974137931034482\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       779\n",
      "           1       1.00      0.99      1.00       381\n",
      "\n",
      "    accuracy                           1.00      1160\n",
      "   macro avg       1.00      1.00      1.00      1160\n",
      "weighted avg       1.00      1.00      1.00      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Modelo de MLP\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=300, random_state=42)\n",
    "mlp_model.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Previsões e Avaliação\n",
    "y_pred_mlp = mlp_model.predict(X_test_tfidf)\n",
    "print(\"MLP - Acurácia:\", accuracy_score(y_test_tfidf, y_pred_mlp))\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_test_tfidf, y_pred_mlp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
